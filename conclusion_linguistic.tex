\subsection{A theory of language \label{Conclusion:Language}}

This section contains first an introduction to the theories regarding the development
of human and animal languages.
It contains a brief summary of the current knowledge about the biological
roots of language formation and experiments which try to replicate artificial
language.

The communication model used in my artificial social system is very basic and mimics essentially
the simple mechanism of signals used in simple animals like primates or event insects.
The power of a more abstract or symbolic language has been assessed in lesion brain
studies where basically it was discovered that intelligence is rooted in language.
The recent discovery of the FOXP2 gene -dubbed the "language gene"- has provided
an astonishing example of the importance of the Broca's functional area;
in humans, mutations of FOXP2 cause a severe speech and language
disorder \citep{FoxP2Gene:Nature:2001,FOXP2Identification:2005}.

A model of language development, was achieved by \citet{Steels:1998:OriginsOntologies,Steels:1999:TalkingHeads}, 
the talking heads experiment shows that a grounded language can indeed be
evolved and contain many properties seen in natural languages like polysemy and
synonyms.
\citet{Steels:1999:TalkingHeads} has investigated how artificial agents can self-organize
languages with natural-language like properties and how meaning can co-evolve
with language. His hypothesis is that language is a
complex adaptive system that emerges through adaptive interactions between
agents and continues to evolve in order to remain adapted to the needs and
capabilities of the agents.
Thus a community of language capable agents can be viewed as a complex adaptive
system which collectively solves the problem of developing a shared
communication system.
To achieve that, the community must reach an agreement on a set of forms
(a sound system in the case of spoken language), a set of meanings (the
conceptualisations of reality), and a set of form-meaning pairs (the
lexicon and grammar).
The experiments implemented interactive robots that were programmed to play language games and
observed the characteristics of the languages that emerged; surprisingly the
agents were able to self organize and develop a common language, which resembled many features of human like
languages, without the help of an external teacher. 
The pre-requisite for the emergence of language is the cognitive and
 sensory-motor ability at the individual level because without the ad-hoc apparatus for
exchanging information and the ability to categorize the environment it is impossible to develop a language.

The experiments shows that for a language to emerge there are several conditions:
\begin{itemize}
 \item a common frame of attention
 \item the ability of perspective change
 \item a reliable system of communication
 \item adaptive learning
\end{itemize}

A sign language can emerge by interacting agents in the world: organisms can develop
an alphabet of actions that will generate a predictive behaviour. Put simply, the agents have mutual 
expectancies from each other: if agent A sees a red square
in front of him, it will produce a tone say at 150 Hz and agent B maybe will produce
a tone say at 300 Hz. With time the agents will use a common alphabet to indicate
different geometric shapes. This is possible because (A) both agents have the same or
similar computational capacities and (B) both agents can imitate each others actions.
Imitation is very important for the development of language and it has been discovered
that this important function is implemented by mirror neurons both in primates and
humans \citep{Buccino200:MirrorNeurons}. Even non primates like birds, must have a brain circuit
generated by the gene PX64 to enable them to reproduce acoustic sounds of similar pitch.
Birds that doesn't have this gene cannot develop a common language, even children who
lack this gene are not able to develop a proper language. It turns out that
if an agent is to be able to learn a language they must have a capacity to imitate and process in memory 
actions performed by his peers.

My current communication model on the contrary is limited or bounded to the properties
of the environment: doing so limits the recursivity of a symbolic system.
I need a higher process that is able to abstract the embodied
actions' that happen in the world to a higher level of actions (let's call them symbols
but they can be sounds or tones) that are able not only to refer to the objects of
the real world but also to each other.

It is very easy to see the limit of an embodied language system: a well known studied
effect is that one of the foraging domestic pigeons (which google sarcastically claims
use a new “page ranking”system). Pigeons do have a good visual system but can
detect food relatively well on the ground, once a pigeon finds a possible source of food
it goes on the ground and randomly samples the ground to find the lucky spot!
Another pigeon flying nearby will see his fellow pigeon wondering around the interesting
spot and with some probability it will go on the ground and look for the food itself!
If we repeat the step many times for each time a pigeon passes nearby we can see an
entire storm of pigeons eating “imaginary” food! This kind of behaviour [reference]
is a positive feedback mechanism which is based on a priory knowledge of the pigeon:
the fact that if a fellow pigeon is looking for food on the ground then there should
be something there! To some extent pigeons base their decisions on Bayesian inference.
Surprisingly for very simple behaviours this also humans base their choices on Bayesian
inference. This was shown in a very simple experiment that you can do when you feel
bored: walk along the street with a friend and at a given time stop and look up at a 
point in the sky. After 1 hour you will get other 20 people doing that and so on.
This primitive communication language happens if, as we said before, all the necessary
conditions are met: a joint frame of attention for the pigeons looking for food on
the ground and for the humans looking at some point in the sky, the ability to
imitate actions (here we suppose to move to the same point in the space) and the
ability to remember what the other are doing. How this a-priory knowledge has
developed could depend on many factors, it's certainly due to the evolution
mechanism as well as on our personal experience. For us it's very hard to get
fooled by that trick more than one time as we may infer that when a group of
people gather in that situation it has no importance, but for the pigeon it is another
story. Pigeons will keep that behaviour because it is beneficial:
on average they will be able to find some food and therefore will keep
their a-priory estimate so that next time they will come back.
For the unlucky pigeon that, by a series of unfortunate events, will not get any food,
there will be a life of solitude and eventually an early retirement!
The conclusion here is that language cannot be based solely on actions because,
they are embodied in the environment where they are performed.
Thus to develop a more complex language, the organism must be provided with
an additional layer of sensory-motor loop designed or adapted for the purpose of communication.


\subsection{Language model and control}
A control model which includes language was used in \citet{Steels:1999:TalkingHeads}
and is summarised in Figure \ref{Fig:Language:Talk}
where two artificial agents play the role of the speaker and the hearer each
time they meet each other in a rule based language game.
The model adopted by Luc Steels includes similar features to Luhmann, the
intentionality is achieved by the goal to reduce uncertainty, the utterance module
also involves the information selection, and parsing the utterance is basically
the understanding process.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.8 \textwidth]{autopoiesis/languageModel}
\end{center}
\small{
\caption[Luc Steels language model]{
Both the speaker and the hearer have a layered architecture:
the language system takes care of the production and parsing of the utterance,
the conceptual system takes care of the understanding,
the sensory-motor system includes the modelling of the world and the intention
or goal of the agent.
\label{Fig:Language:Talk}}}
\end{figure}

In the talking head experiment \citep{Steels:1999:TalkingHeads}, every communication round
is composed of one speaker and one hearer.
The speaker and hearer share a whiteboard (called ``the context``)
full of geometric coloured shapes (triangles, squares, circles).
The speaker, uses image segmentation to choose a topic like ''the green triangle``
or ''the square in the top left corner``.
The speaker then, chooses a word from its dictionary to describe the topic,
then it emits a ''linguistic hint`` to the hearer.
Based on the linguistic hint, the hearer tries to guess what topic the speaker
has chosen, and he communicates his choice to the speaker by pointing to the object.
The hearer points by transmitting in which direction he is looking.
The game is considered successful if the topic guessed by the hearer is equal
to the topic chosen by the speaker.
The game fails if the guess was wrong or if the speaker or the hearer failed at
some earlier point in the game.
In case of failure, the speaker indicates the topic he had in mind, and both
agents try to ''synchronize`` their dictionaries to be more successful in future games.
The talking head experiment shows that a grounded language can indeed be
evolved and contains many properties seen in natural languages, such as polysemy and synonyms.

In my simulations what the agents are missing is the conceptual system layer
of Figure \ref{Fig:Language:Talk} which generated concepts and extract meanings
from the language system.
The world model of my agents is quite simple because it basically reduces a multidimensional
time series in a single integrated value which is the ICO weight.
The world model is of course a bottle neck for the conceptual system but that
does not imply that a simple conceptual layer cannot be developed.
For example part of the plan in my simulations was to introduce such a conceptual layer ( ``telepathy'' feature) whereby the agents shares their own weight with each other.
When the agent receives the weight from one if his fellow, it can decide whether
to use it or not.
If the weight is beneficial, then is going to keep it otherwise is going to reject it.
This simple mechanism allow agents to share their representation of the world
and assigning a simple binary meaning: does it work or not?
With this sort of communication, one would also expect a faster convergence in the
self-organization property because agents does not have to experience everything
but can just try to apply somebody's else knowledge.
This feature unfortunately as explained in Figure \ref{Fig:Language:Telepahty}
was not implemented for the lack of time but would be
certainly improve the model complexity and possibly generate more complex collective
behaviour. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.8 \textwidth]{Telepathy}
\end{center}
\small{
\caption[More advanced language model]{
In this situation there are three agents.
Agent 2 behaviour is to be attracted by agent 3 due to its current weight status $w12$.
Agent 1 is going to learn the food attraction behaviour and consequently
transmitting his weight $w11$ to Agent 2.
Agent 2 will receive the communication from Agent 1 and will decide to try
the new weight $w12$.
If the operation is successful it will keep the new weight $w12$ and overwrite
its previous one $w11$.
\label{Fig:Language:Telepahty}}}
\end{figure}


It is also equally important to study animal language and see what are the
main differences with the human language.
Due to the complexity of the cognitive abilities of humans, in this Thesis
I used models of language closer to animals rather then
humans, and so I am going to describe in the following sections some experimental
evidence of animal language.

\subsection{What is going on in an animal's head?}
What do the signals given by primates, and their responses, tell us about how monkeys think?
When we see an animal do something, it is tempting to assume that it's thinking as we
would if we behaved in the same way. But, as we explain in the context of alarm calls,
this need not to be so. Sometimes, however, by appropriate use of playback experiments,
we can get answers.
\subsection{Do signals convey information about the external world?}
Some signals convey information about the signaller. For example the black and yellow
stripes on a Cinnabar Moth caterpillar carry the message ''I am distasteful', a
fact about the signaller, not about the world external to the signaller.
In our model, agents show to the other, their state of satedness: it's the same concept.
But other signals do carry such information: for example, a bird alarm call carries the
message ``there is a predator close by'' or in my framework, the agent signals the
presence of a food resource in front of it by changing its colour. So what is the difference between the two cases?
It's about what, if anything, goes on in the mind of the receiver of the signal.
Is the receiver genetically programmed to react to the alarm (a pure reactive agent) or
is the receiver formulating a hypothesis of the external world (a non reactive agent)?
To be specific, when a Vervet Monkey hears a Leopard alarm, it climbs a tree.
Does it do so because it forms an image of a Leopard in its mind and behaves accordingly,
or because it follows the behavioural rule ``when you hear that call, climb a tree'' ?
We know that a Vervet will behave appropriately when it hears a Leopard alarm,
even when no Leopard is present.
But what is going on in its head? \citet{Seyfarth2000:AwarenessMonkey} attempted
to answer this question by habituating experiments.
Summarizing their conclusion ``Vervet Monkeys, therefore, appear to interpret
their calls as sounds that represent, or denote, objects and events in the external world''.
Current imaging studies are shedding more light into the mechanism of humanoid brains
 but there is still a lot of unknown processes.

\subsection{Do signallers intend to alter the behaviour of receivers? \label{TheoryOfMind}}
\citet{Moller1998:FalseAlarmCalls} argues that subordinate birds gave false alarms to drive away more dominant
individuals and thereby gain access to food. It requires only that individual birds learn that
giving an alarm note increases their access to food: the calling bird does not
have to think ``if I give an alarm, other birds will think that there is a predator and fly away''.
An even more cautious interpretation is that the behaviour is not learnt at all:
it's innate in all situation in which calling has been selectively favoured in the past.
In my framework some agents cheat to decrease food competition, but to keep the model
simple this behaviour is not learned, only a percentage of the population is
cheating. In this example,
there is no need to assume that an animal ascribes thoughts and beliefs to others.
Humans certainly do. What of other primates?
A summary of Dennett's classification of ``intentionality'' \citep{Dennett1987:Intentional}:
\begin{itemize}
 \item \textit{Zero-order intentionality.} The signaller holds no beliefs or desires:
 a black and yellow caterpillar is a likely example.
 \item \textit{First-order intentionality.} The signaller holds beliefs,
but no beliefs about the beliefs of others.
A Great Tit giving an alarm does not believe that there is a predator (if the signal is honest),
or that the alarm will not increase its access to food (if the signal is a lie),
but in neither case need it have any beliefs about what other Great Tits are thinking.
 \item \textit{Second-order intentionality.}  The signaller ascribes thoughts and beliefs to the receiver.
\end{itemize}
The existence of zero-order and first-order intentionality in animals
should not be controversial. One problem is relevant for the second-order intentionality:
the influence on the signaller of the presence of potential signal hearers.
Vervets and others (including ground squirrels and chickens) do not call when alone.
This shows that animals may be aware of the presence of other individuals before giving an
alarm, but does not require that they ascribes thoughts to others. To summarise,
although animals are influenced, when signalling, by the presence and relatedness of
potential hearers, they do not seem to be influenced in their signals by the
knowledge that hearers might be supposed to possess (e.g. a monkey already giving the
alarm call is supposed to know that a leopard is present) Is there any evidence that
signallers ascribe beliefs to others? A theory  called ``Machiavellian Intelligence''
was formulated by \citet{Byrne1988}. The specific thesis is that
group-living primates have been selected to deceive other group members, and that this
requires that they develop a ``theory of mind'': they are able to ascribe beliefs to others.
There is a general agreement that primates do sometimes send signals which causes others
to behave as if they have been deceived. However, as said before about the hawk alarm call,
 this does not require that the signaller ascribes beliefs to others: it is sufficient that
 the signaller learns by experience that the false signal has the desired effect on
 the receiver's behaviour.
Human children, for example, develops the ability to ascribe beliefs when they are 4 years old
\citep{WimmerPerner1983:ChildMind}.
There is no doubt that some animal signals do potentially carry information
about the external world, and that receivers of such signals respond in a way
that would be appropriate if they had acquired that information. It is much harder
 to decide in particular cases whether the receiver in fact acquires
the information, or whether it merely responds appropriately.
According to the theory of animal signals \citep{AnimalSignals}, signals in my simulations can
 be regarded as:
\begin{enumerate}
\item the field $G_{sated}$ in eq. \ref{eq:gsated} is an index signal, expressing
a quality of the agent (its state of satedness) which cannot be faked.
\item the field $G_{food}$ in eq. \ref{eq:food} could be regarded as a costly
signal or a free signal
\end{enumerate}
I did two different simulations considering, $P_{e}(t)$ as the efficacy cost
needed to ensure that the information can be reliably perceived, $P_{s}(t)$ cost
 needed by the handicap principle \citep{Zahavi1975:MateSelection} to ensure honesty.
Efficacy cost is considered free in my simulation (see section \ref{SocialSystem:Broadcast}):
\begin{enumerate}
\item $G_{food}$ as an honest signal with $P_{e}(t)=0$ and $P_{s}(t)>0$ because competition for food will increase
\item $G_{food}$ as a dishonest signal with $P_{e}(t)=0$ and $P_{s}(t)=0$ because the agent that hasn't food, does not have any disadvantage.
\end{enumerate}
The model that I used is based on the zero order intentionality and one of the aim
for future research is to achieve the second order intentionality.

